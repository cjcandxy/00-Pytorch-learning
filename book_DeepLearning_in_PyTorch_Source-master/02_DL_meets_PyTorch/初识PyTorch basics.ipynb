{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一课 当深度学习遇上PyTorch\n",
    "\n",
    "在这节课中，我们主要展示了PyTorch的使用方法，以及如何用PyTorch实现一个线性回归算法\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、有关Tensor和Autograd变量的练习\n",
    "### 1. Tensor\n",
    "#### a. 产生Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  #导入torch包\n",
    "torch.__version__ #显示当前PyTorch版本号，笔者用的是0.1.12_2，有些命令可能在新的版本下无法执行，请参考PyTorch文件找到最新的相应命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8873, 0.7742, 0.1948],\n",
       "        [0.5196, 0.7902, 0.6178],\n",
       "        [0.0159, 0.7344, 0.8810],\n",
       "        [0.8398, 0.9390, 0.2886],\n",
       "        [0.5916, 0.1302, 0.9575]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)  #产生一个5*3的tensor，随机取值\n",
    "x  #显示x的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(5, 3) #产生一个5*3的Tensor，元素都是1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros(2, 5, 3) #产生一个5*3的Tensor，元素都是1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1155)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0530,  0.1155,  0.7956,  0.9971,  0.0845])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2977,  1.6382,  1.0533],\n",
       "        [ 1.5915,  1.2605,  1.0597],\n",
       "        [ 1.2345,  1.3825,  1.9734],\n",
       "        [ 1.5263,  1.0482,  1.1653],\n",
       "        [ 1.0252,  1.1356,  1.7530]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = torch.FloatTensor([[0.3297,0.7021,0.1119],[0.6668,0.6904,0.1953],[0.6683,0.4260,0.2950],[0.0899,0.4099,0.0882],[0.4675,0.8369,0.1926]])\n",
    "z = x + y #两个tensor可以直接相加\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的语句展示了两个tensor按照矩阵的方式相乘，注意x的尺寸是5*3，y的尺寸也是5*3无法进行矩阵乘法，所以先将y进行转置。\n",
    "转置操作可以用.t来完成，也可以用<!-- lang:python-->.transpose(0, 1)来完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1437,  1.1437,  1.1437,  1.1437,  1.1437],\n",
       "        [ 1.5525,  1.5525,  1.5525,  1.5525,  1.5525],\n",
       "        [ 1.3893,  1.3893,  1.3893,  1.3893,  1.3893],\n",
       "        [ 0.5880,  0.5880,  0.5880,  0.5880,  0.5880],\n",
       "        [ 1.4970,  1.4970,  1.4970,  1.4970,  1.4970]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = x.mm(y.t()) #x乘以y的转置\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Tensor与numpy.ndarray之间的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np #导入numpy包\n",
    "a = np.ones([5, 3]) #建立一个5*3全是1的二维数组（矩阵）\n",
    "b = torch.from_numpy(a) #利用from_numpy将其转换为tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.FloatTensor(a) #另外一种转换为tensor的方法，类型为FloatTensor，还可以使LongTensor，整型数据类型\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()  #从一个tensor转化为numpy的多维数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor和numpy的最大区别在于tensor可以在GPU上运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  #检测本机器上有无GPU可用\n",
    "    x = x.cuda() #返回x的GPU上运算的版本\n",
    "    y = y.cuda()\n",
    "    print(x + y) #tensor可以在GPU上正常运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  #检测本机器上有无GPU可用\n",
    "    device = torch.device(\"cuda\")          # 选择一个CUDA设备\n",
    "    y = torch.ones_like(x, device=device)  # 在GPU上直接创建张量\n",
    "    x = x.to(device)                       # 也可以直接加载``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # 转回到CPU上``.to``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 有关自动微分变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2, 2), requires_grad=True)  #创建一个Variable，包裹了一个2*2张量，将需要计算梯度属性置为True\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x117bde668>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2  #可以按照Tensor的方式进行计算\n",
    "y.grad_fn  #每个Variable都有一个creator（创造者节点）\n",
    "#注：在新版本PyTorch中，可以用.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward1 at 0x10bd3b2b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y  #可以进行各种符合运算\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意，.data可以反回一个Variable所包裹的Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.mean(y * y)  #也可以进行复合运算\n",
    "z.data #.data属性可以返回z所包裹的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** backward可以实施反向传播算法，并计算所有计算图上叶子节点的导数（梯度）信息。注意，由于z和y都不是叶子节点，所以都没有梯度信息）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([[ 1.5000,  1.5000],\n",
      "        [ 1.5000,  1.5000]])\n"
     ]
    }
   ],
   "source": [
    "z.backward() #梯度反向传播\n",
    "print(z.grad)\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的例子中，我们让矩阵x反复作用在向量x上，系统会自动记录中间的依赖关系和长路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor([[0.01, 0.02]], requires_grad = True) #创建一个1*2的Variable（1维向量）\n",
    "x = torch.ones(2, 2, requires_grad = True) #创建一个2*2的矩阵型Variable\n",
    "for i in range(10):\n",
    "    s = s.mm(x)  #反复用s乘以x（矩阵乘法），注意s始终是1*2的Variable\n",
    "z = torch.mean(s) #对s中的各个元素求均值，得到一个1*1的scalar（标量，即1*1张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 37.1200,  37.1200],\n",
      "        [ 39.6800,  39.6800]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "z.backward() #在具有很长的依赖路径的计算图上用反向传播算法计算叶节点的梯度\n",
    "print(x.grad)  #x作为叶节点可以获得这部分梯度信息\n",
    "print(s.grad)  #s不是叶节点，没有梯度信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、利用PyTorch实现简单的线性回归算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 准备数据\n",
    "\n",
    "在这里，我们人为生成一些样本点作为我们的原始数据\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 100).type(torch.FloatTensor) #linspace可以生成0-100之间的均匀的100个数字\n",
    "rand = torch.randn(100) * 10 #随机生成100个满足标准正态分布的随机数，均值为0，方差为1.将这个数字乘以10，标准方差变为10\n",
    "y = x + rand #将x和rand相加，得到伪造的标签数据y。所以(x,y)应能近似地落在y=x这条直线上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[: -10]\n",
    "x_test = x[-10 :]\n",
    "y_train = y[: -10]\n",
    "y_test = y[-10 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将生成的训练数据点画在图上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #导入画图的程序包\n",
    "plt.figure(figsize=(10,8)) #设定绘制窗口大小为10*8 inch\n",
    "plt.plot(x_train.numpy(), y_train.numpy(), 'o') #绘制数据，考虑到x和y都是Variable，需要用data获取它们包裹的Tensor，并专成numpy\n",
    "plt.xlabel('X') #添加X轴的标注\n",
    "plt.ylabel('Y') #添加Y周的标注\n",
    "plt.show() #将图形画在下面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构造模型，计算损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的代码中，需要注意expand_as和mul的使用。首先，a的维度为1，x的维度为100*1的Tensor，这两者不能直接相乘，因为维度不同。\n",
    "\n",
    "所以，先要将a升维成1*1的Tensor。这就好比将原本在直线上的点被升维到了二维平面上，同时直线仍然在二维平面中。\n",
    "\n",
    "```expand_as(x)```可以将张量升维成与x同维度的张量。所以如果a = 1, x为尺寸为100，那么，\n",
    "\n",
    "a.expand_as(x)$ = (1, 1, \\cdot\\cdot\\cdot, 1)^T$\n",
    "\n",
    "```x * y```为两个1维张量的乘积，计算结果：\n",
    "\n",
    "$(x * y)_i = x_i \\cdot y_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4550,  1.1246,  1.7942,  2.4638,  3.1334,  3.8030,  4.4726,  5.1422,\n",
       "         5.8118,  6.4814,  7.1510,  7.8206,  8.4902,  9.1599,  9.8295, 10.4991,\n",
       "        11.1687, 11.8383, 12.5079, 13.1775, 13.8471, 14.5167, 15.1863, 15.8559,\n",
       "        16.5255, 17.1951, 17.8647, 18.5343, 19.2039, 19.8735, 20.5431, 21.2128,\n",
       "        21.8824, 22.5520, 23.2216, 23.8912, 24.5608, 25.2304, 25.9000, 26.5696,\n",
       "        27.2392, 27.9088, 28.5784, 29.2480, 29.9176, 30.5872, 31.2568, 31.9265,\n",
       "        32.5961, 33.2657, 33.9353, 34.6049, 35.2745, 35.9441, 36.6137, 37.2833,\n",
       "        37.9529, 38.6225, 39.2921, 39.9617, 40.6313, 41.3009, 41.9705, 42.6401,\n",
       "        43.3097, 43.9794, 44.6490, 45.3186, 45.9882, 46.6578, 47.3274, 47.9970,\n",
       "        48.6666, 49.3362, 50.0058, 50.6754, 51.3450, 52.0146, 52.6842, 53.3538,\n",
       "        54.0234, 54.6930, 55.3627, 56.0323, 56.7019, 57.3715, 58.0411, 58.7107,\n",
       "        59.3803, 60.0499], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad = True)\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "predictions = a.expand_as(x_train) * x_train + b.expand_as(x_train)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((predictions - y_train) ** 2)  #计算损失函数\n",
    "loss.backward() #开始反向传播梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4848])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#开始梯度下降，其中0.001为学习率\n",
    "a.data.add_(- 0.001 * a.grad.data) \n",
    "b.data.add_(- 0.001 * b.grad.data)\n",
    "\n",
    "#注意我们无法改变一个Variable，而只能对Variable的data属性做更改\n",
    "#所有函数加“_”都意味着需要更新调用者的数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练模型的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 错误版本\n",
    "\n",
    "错误在于，每一步迭代周期没有将a和b的梯度（grad）数值设置为0，导致每一步backward候梯度就会不断累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters: tensor([0.2850], requires_grad=True) tensor([0.9557], requires_grad=True)\n",
      "loss: tensor(1479.8816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(410.4522, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(275.8459, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1328.3325, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1595.4119, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(576.5415, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(181.2191, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1150.3303, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1667.6317, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(763.6307, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(132.5446, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(957.1117, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.9827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(959.9110, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(132.8944, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(760.8710, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1666.9283, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1152.9927, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(182.2463, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(573.9947, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1594.0476, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1330.6884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(277.4852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(408.2784, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1477.9415, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1481.7820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(412.5994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(274.1815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1325.9391, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1596.7365, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(579.0602, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(180.1681, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1147.6334, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1668.2948, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(766.3606, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(132.1717, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(954.2789, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.9407, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(962.6774, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(133.2216, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(758.0800, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1666.1809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1155.6188, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(183.2507, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(571.4199, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1592.6411, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1333.0060, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(279.1009, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(406.0794, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1475.9626, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1483.6428, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(414.7218, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(272.4947, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1323.5094, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1598.0200, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(581.5521, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(179.0966, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1144.9041, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1668.9175, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(769.0611, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(131.7800, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(951.4180, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.8595, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(965.4130, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(133.5305, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(755.2642, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1665.3969, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1158.2135, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(184.2370, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(568.8230, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1591.2009, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1335.2928, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(280.6985, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(403.8618, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1473.9519, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1485.4716, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(416.8252, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(270.7919, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1321.0509, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1599.2712, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(584.0244, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(178.0115, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1142.1481, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1669.5074, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(771.7416, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(131.3763, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(948.5339, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.7467, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(968.1279, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(133.8291, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(752.4301, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1664.5837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1160.7865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(185.2141, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(566.2122, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1589.7330, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1337.5553, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(282.2871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(401.6336, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1471.9169, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1487.2767, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(418.9202, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(269.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1318.5720, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1600.4996, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(586.4880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(176.9223, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1139.3761, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1670.0762, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(774.4130, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(130.9713, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(945.6392, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.6138, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(970.8330, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(134.1282, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(749.5883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1663.7522, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1163.3484, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(186.1930, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(563.5978, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1588.2493, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1339.8073, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(283.8783, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(399.4062, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1469.8694, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1489.0707, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(421.0177, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(267.3761, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1316.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1601.7163, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(588.9538, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(175.8406, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1136.5990, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1670.6335, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(777.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(130.5763, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(942.7430, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.4708, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(973.5381, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(134.4393, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(746.7504, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1662.9127, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1165.9099, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(187.1849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(560.9922, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1586.7611, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1342.0582, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(285.4832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(397.1917, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1467.8214, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1490.8643, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(423.1287, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(265.6872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1313.5994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1602.9333, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(591.4329, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(174.7785, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1133.8304, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1671.1929, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(779.7712, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(130.2031, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(939.8600, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.3318, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(976.2559, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(134.7738, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(743.9299, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1662.0797, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1168.4835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(188.2014, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(558.4073, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1585.2816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1344.3207, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(287.1128, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(395.0013, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1465.7844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1492.6681, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(425.2646, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(264.0255, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1311.1300, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1604.1611, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(593.9361, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(173.7463, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1131.0804, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1671.7632, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(782.4802, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(129.8619, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(936.9986, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.2052, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(978.9957, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(135.1416, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(741.1349, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1661.2607, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1171.0782, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(189.2516, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(555.8519, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1583.8191, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1346.6034, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(288.7763, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(392.8439, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1463.7673, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1494.4922, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(427.4338, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(262.3994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1308.6835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1605.4084, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(596.4713, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(172.7520, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1128.3561, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1672.3536, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(785.2196, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(129.5600, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(934.1669, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.0994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(981.7646, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(135.5494, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(738.3725, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1660.4625, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1173.7002, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(190.3422, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(553.3320, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1582.3787, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1348.9121, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(290.4796, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(390.7245, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1461.7747, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1496.3403, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(429.6412, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(260.8138, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1306.2637, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1606.6797, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(599.0435, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(171.7997, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1125.6611, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1672.9659, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(787.9938, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(129.3011, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(931.3665, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1691.0156, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(984.5658, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(136.0005, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(735.6453, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1659.6882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1176.3523, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(191.4752, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(550.8492, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1580.9618, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1351.2479, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(292.2245, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(388.6444, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1459.8071, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1498.2146, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(431.8888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(259.2689, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1303.8706, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1607.9745, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(601.6531, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(170.8894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1122.9950, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1673.6012, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(790.8025, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(129.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(928.5987, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.9526, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(987.3982, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(136.4936, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(732.9515, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1658.9346, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1179.0321, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(192.6492, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(548.4030, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1579.5679, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1353.6084, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(294.0075, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(386.6024, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1457.8634, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1500.1101, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(434.1716, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(257.7629, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1301.5024, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1609.2887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(604.2949, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(170.0181, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1120.3562, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1674.2544, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(793.6396, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.9063, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(925.8583, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.9078, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(990.2562, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(137.0234, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(730.2870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1658.1978, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1181.7339, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(193.8578, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(545.9868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1578.1904, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1355.9875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(295.8225, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(384.5915, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1455.9360, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1502.0203, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(436.4829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(256.2887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1299.1517, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1610.6147, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(606.9614, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(169.1777, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1117.7352, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1674.9181, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(796.4977, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.7580, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(923.1377, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.8708, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(993.1305, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(137.5815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(727.6431, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1657.4686, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1184.4482, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(195.0922, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(543.5917, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1576.8195, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1358.3765, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(297.6609, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(382.6013, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1454.0149, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1503.9375, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(438.8147, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(254.8346, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1296.8073, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1611.9448, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(609.6448, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(168.3576, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1115.1210, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1675.5819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(799.3692, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.6290, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(920.4244, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.8328, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(996.0134, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(138.1572, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(725.0073, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1656.7356, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1187.1669, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(196.3420, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(541.2056, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1575.4445, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1360.7646, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(299.5115, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(380.6211, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1452.0905, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1505.8505, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(441.1546, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(253.3911, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1294.4604, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1613.2676, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(612.3317, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(167.5468, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1112.5046, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1676.2349, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(802.2389, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.5076, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(917.7093, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.7816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(998.8907, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(138.7384, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(722.3703, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1655.9893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1189.8756, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(197.5944, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(538.8182, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1574.0547, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1363.1389, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(301.3612, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(378.6398, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1450.1501, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1507.7454, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(443.4902, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(251.9457, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1292.0975, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1614.5681, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(615.0104, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(166.7335, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1109.8730, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1676.8658, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(805.0970, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(128.3825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(914.9801, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.7064, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1001.7520, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(139.3140, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(719.7202, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1655.2167, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1192.5642, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(198.8388, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(536.4186, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1572.6384, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1365.4891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(303.2003, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(376.6461, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1448.1835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1509.6130, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(445.8116, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(250.4878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1289.7079, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1615.8392, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(617.6718, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(165.9074, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1107.2156, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1677.4629, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(807.9330, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.2439, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(912.2260, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.5957, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1004.5880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(139.8747, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(717.0463, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1654.4092, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1195.2233, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(200.0663, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(533.9969, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1571.1874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1367.8066, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(305.0197, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(374.6317, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1446.1823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1511.4452, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(448.1106, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(249.0101, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1287.2859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1617.0728, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(620.3073, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(165.0616, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1104.5275, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1678.0217, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(810.7407, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(128.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(909.4424, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.4460, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1007.3916, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(140.4143, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(714.3447, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1653.5613, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1197.8469, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(201.2713, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(531.5485, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1569.6949, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1370.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(306.8148, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(372.5919, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1444.1420, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1513.2368, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(450.3828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(247.5082, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1284.8259, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1618.2639, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(622.9135, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(164.1923, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1101.8030, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1678.5374, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(813.5170, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(127.9030, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(906.6252, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.2526, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1010.1616, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(140.9305, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(711.6115, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1652.6698, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1200.4347, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(202.4522, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(529.0713, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1568.1616, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1372.3274, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(308.5847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(370.5256, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1442.0614, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1514.9888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(452.6287, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(245.9816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1282.3275, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1619.4136, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(625.4916, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(163.2999, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1099.0433, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1679.0104, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(816.2619, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(127.6990, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(903.7757, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1690.0168, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1012.8980, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(141.4251, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(708.8497, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1651.7378, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1202.9878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(203.6115, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(526.5686, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1566.5900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1374.5321, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(310.3323, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(368.4369, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1439.9448, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1516.7032, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(454.8509, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(244.4353, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1279.7979, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1620.5262, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(628.0445, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(162.3897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1096.2545, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1679.4469, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(818.9810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(127.4783, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(900.9009, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1689.7462, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1015.6075, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(141.9039, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(706.0662, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1650.7729, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1205.5123, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(204.7556, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(524.0469, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1564.9861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1376.7087, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(312.0649, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(366.3323, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1437.8003, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1518.3893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(457.0580, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(242.8756, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1277.2408, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1621.6097, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(630.5821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(161.4689, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1093.4435, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1679.8547, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(821.6834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(127.2495, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(898.0074, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1689.4479, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1018.2991, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(142.3763, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(703.2680, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1649.7820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1208.0179, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(205.8937, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(521.5159, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1563.3590, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1378.8639, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(313.7918, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(364.2218, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1435.6345, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1520.0533, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(459.2592, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(241.3138, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1274.6687, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1622.6724, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(633.1128, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(160.5488, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1090.6206, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1680.2433, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(824.3785, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(127.0235, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(895.1075, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1689.1320, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1020.9827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(142.8531, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(700.4678, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1648.7755, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1210.5154, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(207.0376, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(518.9863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1561.7216, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1381.0123, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(315.5248, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(362.1173, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1433.4625, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1521.7109, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(461.4665, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(239.7616, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1272.0942, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1623.7290, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(635.6495, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(159.6411, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1087.8003, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1680.6274, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(827.0793, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.8122, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(892.2133, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1688.8134, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1023.6719, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(143.3463, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(697.6774, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1647.7697, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1213.0178, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(208.1988, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(516.4708, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1560.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1383.1648, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(317.2758, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(360.0307, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1431.2952, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1523.3721, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(463.6914, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(238.2304, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1269.5283, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1624.7902, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(638.2035, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(158.7571, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1084.9922, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1681.0161, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(829.7959, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.6267, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(889.3356, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1688.5004, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1026.3757, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(143.8665, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(694.9078, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1646.7710, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1215.5338, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(209.3879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(513.9803, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1558.4604, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1385.3298, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(319.0541, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(357.9725, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1429.1428, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1525.0464, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(465.9438, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(236.7304, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1266.9788, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1625.8639, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(640.7834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(157.9067, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1082.2053, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1681.4186, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(832.5383, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.4763, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(886.4837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1688.2032, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1029.1036, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(144.4228, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(692.1677, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1645.7896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1218.0723, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(210.6131, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(511.5228, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1556.8549, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1387.5165, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(320.8687, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(355.9496, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1427.0110, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1526.7410, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(468.2307, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(235.2684, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1264.4556, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1626.9580, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(643.3975, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(157.0963, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1079.4473, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1681.8420, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(835.3126, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.3671, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(883.6635, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1687.9276, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1031.8615, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(145.0207, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(689.4626, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1644.8311, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1220.6393, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(211.8796, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(509.1022, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1555.2731, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1389.7294, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(322.7235, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(353.9667, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1424.9064, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1528.4608, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(470.5566, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(233.8481, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1261.9598, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1628.0757, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(646.0479, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(156.3286, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1076.7186, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1682.2874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(838.1208, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.3013, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(880.8756, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1687.6736, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1034.6516, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(145.6619, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(686.7922, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1643.8948, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1223.2355, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(213.1885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(506.7193, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1553.7151, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1391.9705, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(324.6195, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(352.0231, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1422.8275, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1530.2064, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(472.9214, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(232.4686, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1259.4921, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1629.2180, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(648.7345, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(155.6025, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1074.0206, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1682.7556, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(840.9616, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.2769, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(878.1199, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1687.4424, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1037.4709, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(146.3434, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(684.1556, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1642.9814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1225.8586, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(214.5362, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(504.3712, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1552.1801, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1394.2354, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(326.5518, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(350.1157, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1420.7709, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1531.9729, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(475.3200, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(231.1256, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1257.0482, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1630.3779, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(651.4512, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(154.9127, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1071.3473, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1683.2413, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(843.8300, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.2878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(875.3904, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1687.2266, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1040.3137, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(147.0588, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(681.5463, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1642.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1228.5012, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(215.9154, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(502.0517, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1550.6591, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1396.5148, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(328.5123, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(348.2368, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1418.7291, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1533.7507, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(477.7430, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(229.8113, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1254.6194, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1631.5480, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(654.1895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(154.2505, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1068.6903, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1683.7340, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(846.7151, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.3251, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(872.6775, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1687.0156, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1043.1691, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(147.7984, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(678.9545, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1641.1874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1231.1520, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(217.3161, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(499.7497, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1549.1420, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1398.7994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(330.4912, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(346.3760, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1416.6909, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1535.5308, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(480.1808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(228.5142, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1252.1942, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1632.7162, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(656.9379, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(153.6050, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1066.0367, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1684.2222, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(849.6063, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.3774, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(869.9688, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.7991, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1046.0260, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(148.5510, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(676.3681, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1640.2845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1233.7998, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(218.7269, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(497.4539, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1547.6167, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1401.0768, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(332.4767, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(344.5212, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1414.6438, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1537.2993, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(482.6214, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(227.2230, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1249.7610, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1633.8698, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(659.6846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(152.9643, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1063.3756, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1684.6937, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(852.4919, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.4331, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(867.2540, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.5636, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1048.8727, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(149.3047, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(673.7750, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1639.3623, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1236.4330, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(220.1362, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(495.1523, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1546.0698, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1403.3354, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(334.4575, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(342.6606, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1412.5764, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1539.0458, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(485.0534, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(225.9259, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1247.3080, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1634.9984, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(662.4193, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(152.3172, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1060.6964, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.1378, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(855.3602, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.4811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(864.5209, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.2994, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1051.6991, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(150.0490, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(671.1656, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1638.4109, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1239.0428, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(221.5338, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(492.8343, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1544.4946, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1405.5675, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(336.4239, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(340.7846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1410.4802, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1540.7625, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(487.4680, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(224.6135, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1244.8259, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1636.0945, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(665.1331, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(151.6543, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1057.9882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.5470, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(858.2044, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.5127, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(861.7614, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.9989, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1054.4968, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(150.7756, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(668.5302, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1637.4215, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1241.6204, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(222.9118, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(490.4925, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1542.8821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1407.7637, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(338.3683, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(338.8859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1408.3474, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1542.4404, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(489.8578, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(223.2792, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1242.3092, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1637.1504, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(667.8186, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(150.9701, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1055.2477, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.9152, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(861.0177, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.5225, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(858.9709, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.6570, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1057.2606, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(151.4793, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(665.8668, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1636.3917, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1244.1604, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(224.2656, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(488.1242, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1541.2290, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1409.9208, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(340.2866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(336.9621, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1406.1758, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1544.0776, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(492.2195, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(221.9212, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1239.7559, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1638.1639, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(670.4739, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(150.2624, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1052.4728, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.2405, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(863.7982, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.5090, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(856.1479, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1685.2727, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1059.9900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(152.1600, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(663.1733, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1635.3199, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1246.6644, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(225.5955, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(485.7283, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1539.5358, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1412.0389, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(342.1804, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(335.0127, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1403.9652, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1545.6750, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(494.5550, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(220.5393, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1237.1654, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1639.1375, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(673.1012, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(149.5331, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1049.6643, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.5248, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(866.5488, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.4752, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(853.2955, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1684.8466, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1062.6869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(152.8206, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(660.4533, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1634.2084, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1249.1343, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(226.9055, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(483.3090, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1537.8051, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1414.1227, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(344.0534, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(333.0432, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1401.7213, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1547.2368, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(496.8689, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(219.1401, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1234.5449, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1640.0741, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(675.7058, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(148.7884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1046.8289, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1686.7739, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(869.2755, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.4276, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(850.4198, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1684.3885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1065.3590, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(153.4685, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(657.7136, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1633.0654, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1251.5786, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(228.2034, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(480.8741, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1536.0455, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1416.1796, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(345.9142, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(331.0621, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1399.4514, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1548.7721, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(499.1702, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(217.7324, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1231.9038, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1640.9851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(678.2971, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(148.0375, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1043.9762, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1686.9983, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(871.9879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.3758, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(847.5310, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1683.9058, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1068.0164, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(154.1136, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(654.9659, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1631.9028, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1254.0081, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(229.4993, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(478.4348, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1534.2694, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1418.2218, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(347.7735, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(329.0800, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(1397.1683, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad = True)\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "print('Initial parameters:', a, b)\n",
    "learning_rate = 0.0001\n",
    "for i in range(1000):\n",
    "    predictions = a.expand_as(x_train) * x_train + b.expand_as(x_train)\n",
    "    loss = torch.mean((predictions - y_train) ** 2)\n",
    "    print('loss:', loss)\n",
    "    loss.backward()\n",
    "    a.data.add_(- learning_rate * a.grad.data)\n",
    "    b.data.add_(- learning_rate * b.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印输出的loss结果来看，存在着非常大的震荡，从而导致无法正确估计参数a和b的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 正确版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters: [tensor([0.1670], requires_grad=True), tensor([0.1301], requires_grad=True)]\n",
      "loss: tensor(2030.7397, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(525.6788, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(209.9223, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(143.6778, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(129.7799, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.8642, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.2525, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.1241, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0972, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0915, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0903, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0901, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0900, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0899, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0898, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0897, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0896, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0895, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0894, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0893, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0892, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0891, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0890, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0889, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0888, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0887, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0886, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0885, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0884, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0883, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0882, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0881, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0880, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0879, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0878, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0877, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0876, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0875, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0874, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0873, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0872, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0871, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0870, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0869, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0868, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0867, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0866, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0865, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0864, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0863, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0862, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0861, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0860, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0859, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0858, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0857, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0856, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0855, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0854, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0853, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0852, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0851, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0850, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0849, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0848, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0847, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0846, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0845, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0844, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0843, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0842, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0841, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0840, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0839, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0838, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0837, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0836, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0835, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0834, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0833, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0832, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0831, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0830, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0829, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0828, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0827, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0826, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0825, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0824, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0823, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0822, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0821, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0820, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0819, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0818, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0817, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0816, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0815, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0814, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0813, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0812, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0811, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0810, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0809, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0808, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n",
      "loss: tensor(126.0807, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad = True) #创建a变量，并随机赋值初始化\n",
    "b = torch.rand(1, requires_grad = True) #创建b变量，并随机赋值初始化\n",
    "print('Initial parameters:', [a, b])\n",
    "learning_rate = 0.0001 #设置学习率\n",
    "for i in range(1000):\n",
    "    predictions = a.expand_as(x_train) * x_train + b.expand_as(x_train)  #计算在当前a、b条件下的模型预测数值\n",
    "    loss = torch.mean((predictions - y_train) ** 2) #通过与标签数据y比较，计算误差\n",
    "    print('loss:', loss)\n",
    "    loss.backward() #对损失函数进行梯度反传\n",
    "    a.data.add_(- learning_rate * a.grad.data)  #利用上一步计算中得到的a的梯度信息更新a中的data数值\n",
    "    b.data.add_(- learning_rate * b.grad.data)  #利用上一步计算中得到的b的梯度信息更新b中的data数值\n",
    "    ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加\n",
    "    a.grad.data.zero_() #清空a的梯度数值\n",
    "    b.grad.data.zero_() #清空b的梯度数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAGtCAYAAACiIojTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw5jcvfwhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117789d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = x_train.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y_train.numpy(), 'o') # 绘制原始数据\n",
    "yplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 91.5473,  92.5632,  93.5791,  94.5951,  95.6110,  96.6269,  97.6429,\n",
       "         98.6588,  99.6747, 100.6907], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = a.expand_as(x_test) * x_test + b.expand_as(x_test) #计算模型的预测结果\n",
    "predictions #输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-27937a4c395c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 绘制训练数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 绘制测试数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#绘制拟合数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#绘制预测数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = x_train.data.numpy() # 获得x包裹的数据\n",
    "x_pred = x_test.data.numpy()\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "plt.plot(x_data, y_train.data.numpy(), 'o') # 绘制训练数据\n",
    "plt.plot(x_pred, y_test.data.numpy(), 's') # 绘制测试数据\n",
    "x_data = np.r_[x_data, x_test.data.numpy()]\n",
    "plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  #绘制拟合数据\n",
    "plt.plot(x_pred, a.data.numpy() * x_pred + b.data.numpy(), 'o') #绘制预测数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第II课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(np.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
